"""
DPO Training Script (Radical Tuning for Arithmetic Fix)
Dataset: cot_rl_train.jsonl
dependence:
accelerate               0.30.1
datasets                 2.19.1
peft                     0.11.1
torch                    2.9.1
transformers             4.41.2
trl                      0.9.6

run with: python rl/train_rl_dpo.py --output_dir results/rl_dpo --per_device_train_batch_size 4 --gradient_accumulation_steps 4
"""

import json
import torch
from dataclasses import dataclass, field
from typing import Optional, Dict, List
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    HfArgumentParser
)
from trl import DPOTrainer, DPOConfig
from tqdm import tqdm

@dataclass
class ScriptArguments:
    model_name_or_path: str = field(default="google/flan-t5-base", metadata={"help": "Model name"})
    train_data_path: str = field(default="data/cot_rl_train.jsonl", metadata={"help": "Path to your REAL training data"})

def load_jsonl(file_path):
    print(f"Loading data from {file_path}...")
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data

def build_preference_dataset(data, tokenizer, model, device):
    """
    Build DPO Dataset:
    Chosen = Your high-quality Output
    Rejected = "Hallucinated" answer generated by the model itself
    """
    print(f"Building DPO dataset (Generating negatives)...")

    formatted_data = {
        "prompt": [],
        "chosen": [],
        "rejected": []
    }

    model.eval()
    model.to(device)

    batch_size = 8

    data_slice = data

    for i in tqdm(range(0, len(data_slice), batch_size)):
        batch_items = data_slice[i : i + batch_size]
        prompts = [item["input"] for item in batch_items]
        targets = [item["output"] for item in batch_items]

        inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)

        # Rejected
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=128,
                do_sample=True,
                temperature=0.7,
                top_k=50
            )

        generated_responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)

        for prompt, target, generated in zip(prompts, targets, generated_responses):
            if generated.strip() == target.strip():
                continue

            formatted_data["prompt"].append(prompt)
            formatted_data["chosen"].append(target)
            formatted_data["rejected"].append(generated)

    print(f"Dataset ready. Effective DPO Samples: {len(formatted_data['prompt'])}")
    return Dataset.from_dict(formatted_data)

def main():
    parser = HfArgumentParser((ScriptArguments, DPOConfig))
    script_args, training_args = parser.parse_args_into_dataclasses()

    if training_args.num_train_epochs == 3.0:
        training_args.num_train_epochs = 10
        print(">>> [Auto-Tune] Increasing Epochs to 10 for better memorization.")

    if training_args.learning_rate == 0.0:
        training_args.learning_rate = 5e-7
        print(">>> [Auto-Tune] Setting LR to 5e-7 for stability.")

    if training_args.beta == 0.1:
        training_args.beta = 0.01
        print(">>> [Auto-Tune] Setting Beta to 0.01 (Radical Mode) to fix hallucinations.")

    training_args.is_encoder_decoder = True

    if training_args.output_dir is None:
        training_args.output_dir = "results/rl_dpo"

    print(f"Loading model: {script_args.model_name_or_path}")
    tokenizer = AutoTokenizer.from_pretrained(script_args.model_name_or_path)
    model = AutoModelForSeq2SeqLM.from_pretrained(script_args.model_name_or_path)
    ref_model = AutoModelForSeq2SeqLM.from_pretrained(script_args.model_name_or_path)
    raw_data = load_jsonl(script_args.train_data_path)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    train_dataset = build_preference_dataset(raw_data, tokenizer, model, device)

    print("Initializing DPO Trainer...")

    trainer = DPOTrainer(
        model=model,
        ref_model=ref_model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        max_length=training_args.max_length or 512,
        max_prompt_length=training_args.max_prompt_length or 512,
        max_target_length=training_args.max_target_length or 128,
    )

    # train
    print("Starting Training (Radical Mode)...")
    trainer.train()

    print(f"Saving model to {training_args.output_dir}")
    trainer.save_model(training_args.output_dir)
    tokenizer.save_pretrained(training_args.output_dir)

if __name__ == "__main__":
    main()